{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avila data set has been extracted from 800 images of the the \"Avila Bible\", a giant Latin copy of the whole Bible produced during the XII century between Italy and Spain.  \n",
    "The palaeographic analysis of the  manuscript has  individuated the presence of 12 copyists. The pages written by each copyist are not equally numerous. \n",
    "Each pattern contains 10 features and corresponds to a group of 4 consecutive rows.\n",
    "\n",
    "The prediction task consists in associating each pattern to one of the 12 copyists (labeled as: A, B, C, D, E, F, G, H, I, W, X, Y).\n",
    "The data have has been normalized, by using the Z-normalization method, and divided in two data sets: a training set containing 10430 samples, and a test set  containing the 10437 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class distribution (training set)\n",
    "A: 4286\n",
    "B: 5  \n",
    "C: 103 \n",
    "D: 352 \n",
    "E: 1095 \n",
    "F: 1961 \n",
    "G: 446 \n",
    "H: 519\n",
    "I: 831\n",
    "W: 44\n",
    "X: 522 \n",
    "Y: 266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTRIBUTE DESCRIPTION\n",
    "\n",
    "ID      Name    \n",
    "F1       intercolumnar distance \n",
    "F2       upper margin \n",
    "F3       lower margin \n",
    "F4       exploitation \n",
    "F5       row number \n",
    "F6       modular ratio \n",
    "F7       interlinear spacing \n",
    "F8       weight \n",
    "F9       peak number \n",
    "F10     modular ratio/ interlinear spacing\n",
    "Class: A, B, C, D, E, F, G, H, I, W, X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.read_csv('avila-tr.txt', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_test = pd.read_csv('avila-ts.txt', header = None, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_test.columns = [\"intercolumnar distance\", \"upper margin\", \"lower margin\", \"exploitation\", \"row number\", \"modular ratio\",\n",
    "                        \"interlinear spacing\", \"weight\", \"peak number\", \"modular ratio/ interlinear spacing\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = database.rename(columns={\"intercolumnar distance\": \"F1\", \"upper margin\": \"F2\", \"lower margin\": \"F3\", \"exploitation\": \"F4\",\n",
    "                        \"row number\": \"F5\", \"modular ratio\": \"F6\", \"interlinear spacing\": \"F7\", \"weight\": \"F8\", \n",
    "                        \"peak number\": \"F9\", \"modular ratio/ interlinear spacing\": \"F10\", \" class\": \"class\"})\n",
    "database_test = database_test.rename(columns={\"intercolumnar distance\": \"F1\", \"upper margin\": \"F2\", \"lower margin\": \"F3\", \"exploitation\": \"F4\",\n",
    "                        \"row number\": \"F5\", \"modular ratio\": \"F6\", \"interlinear spacing\": \"F7\", \"weight\": \"F8\", \n",
    "                        \"peak number\": \"F9\", \"modular ratio/ interlinear spacing\": \"F10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'class'], dtype='object'),\n",
       " Index(['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'class'], dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.columns, database_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(F1       float64\n",
       " F2       float64\n",
       " F3       float64\n",
       " F4       float64\n",
       " F5       float64\n",
       " F6       float64\n",
       " F7       float64\n",
       " F8       float64\n",
       " F9       float64\n",
       " F10      float64\n",
       " class     object\n",
       " dtype: object,\n",
       " F1       float64\n",
       " F2       float64\n",
       " F3       float64\n",
       " F4       float64\n",
       " F5       float64\n",
       " F6       float64\n",
       " F7       float64\n",
       " F8       float64\n",
       " F9       float64\n",
       " F10      float64\n",
       " class     object\n",
       " dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.dtypes, database_test.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On constate que les variables sont premièrement de même type entre train et test et surtout que l'ensemble des variables sont des float à l'exception d'une : 'class' notre variable de prédiction. On peut donc annoncer dès lors que notre problème sera celui d'une classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10', 'class'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On split le dataset en détachant la variable de prédiction afin de bien différencier X et Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = database['class']\n",
    "x = database.drop('class', axis = 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test valeurs NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Il n'y a donc aucune valeur NA : il n'y a pas de nettoyage à faire à ce niveau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00169849,  0.03980698,  0.00153056, -0.00548555,  0.00890724,\n",
       "         0.02280871,  0.00851373,  0.00462168,  0.01596709,  0.0057365 ]),\n",
       " array([0.99804695, 4.46715465, 1.15367818, 1.0197531 , 0.98830433,\n",
       "        1.16779084, 1.3728213 , 0.99328777, 1.11492302, 1.00168716]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.27174614e-17, -2.27097525e-18,  5.45034060e-18, -2.27097525e-17,\n",
       "         1.27174614e-17,  1.27174614e-17,  9.08390099e-18,  2.72517030e-18,\n",
       "        -8.62970594e-18, -1.77136069e-17]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.mean(axis = 0), x_train.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On a bien normalisé nos valeurs afin de nous assurer que les différences d'échelle entre les variables n'influent pas sur notre résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quent\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve, cross_val_score, GridSearchCV\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def test_model(model, X, y):\n",
    "  \"\"\"\n",
    "  Tester un modele\n",
    "  \"\"\"\n",
    "  accuracy = cross_val_score(model, X, y, scoring='accuracy', verbose=1)\n",
    "  avg_accuracy = accuracy.mean()\n",
    "\n",
    "  return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def train_model(model, params, X, y):\n",
    "  \"\"\"\n",
    "  Entrainer un modele\n",
    "  \"\"\"\n",
    "  grid = GridSearchCV(model, params, verbose=1)\n",
    "\n",
    "  grid.fit(X, y)\n",
    "\n",
    "  best_params = grid.best_params_\n",
    "  best_model = grid.best_estimator_\n",
    "\n",
    "  return best_params, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "knn_accuracy = test_model(knn_model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision:\t 0.6807736368613287\n"
     ]
    }
   ],
   "source": [
    "print(f'Précision:\\t', knn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(knn_model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
